{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T16:26:56.771249Z",
     "start_time": "2024-12-28T16:20:38.900595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adversarial Training and Model Robustness Study\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.utils import load_cifar10\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    \"\"\"Generate random bounding box coordinates for CutMix.\n",
    "\n",
    "    Args:\n",
    "        size: Tensor size (B, C, W, H)\n",
    "        lam: Lambda value for computing cut ratio\n",
    "\n",
    "    Returns:\n",
    "        Bounding box coordinates (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# 1. Data Loading and Preprocessing\n",
    "class DatasetPreparation:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        self.transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "    def load_data(self):\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=True, download=True, transform=self.transform_train)\n",
    "        trainloader = DataLoader(\n",
    "            trainset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=False, download=True, transform=self.transform_test)\n",
    "        testloader = DataLoader(\n",
    "            testset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        return trainloader, testloader\n",
    "\n",
    "# 2. Model Definition\n",
    "class ModelSetup:\n",
    "    def __init__(self, num_classes=10):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(pretrained=True)\n",
    "        # Modify the final layer for CIFAR-10\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def get_art_model(self, model, loss_fn, optimizer):\n",
    "        return PyTorchClassifier(\n",
    "            model=model,\n",
    "            loss=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            input_shape=(3, 32, 32),\n",
    "            nb_classes=10\n",
    "        )\n",
    "\n",
    "# 3. Training Functions\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, trainloader, testloader, writer):\n",
    "        self.model = model\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.writer = writer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=200)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        self.writer.add_scalar('Train/Loss', running_loss/(batch_idx+1), epoch)\n",
    "        self.writer.add_scalar('Train/Accuracy', accuracy, epoch)\n",
    "        return running_loss/(batch_idx+1), accuracy\n",
    "\n",
    "# 4. Adversarial Attack Implementation\n",
    "class AdversarialAttacks:\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def create_fgsm_attack(self, eps=0.3):\n",
    "        return FastGradientMethod(estimator=self.classifier, eps=eps)\n",
    "\n",
    "    def create_pgd_attack(self, eps=0.3, eps_step=0.1, max_iter=40):\n",
    "        return ProjectedGradientDescent(\n",
    "            estimator=self.classifier,\n",
    "            eps=eps,\n",
    "            eps_step=eps_step,\n",
    "            max_iter=max_iter,\n",
    "            targeted=False\n",
    "        )\n",
    "\n",
    "# 5. Robustness Evaluation\n",
    "class RobustnessEvaluator:\n",
    "    def __init__(self, model, testloader, writer):\n",
    "        self.model = model\n",
    "        self.testloader = testloader\n",
    "        self.writer = writer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def evaluate_clean(self, epoch):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.testloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        self.writer.add_scalar('Test/Clean_Accuracy', accuracy, epoch)\n",
    "        return accuracy\n",
    "\n",
    "    def evaluate_adversarial(self, attack, epoch):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in self.testloader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            adv_inputs = attack.generate(x=inputs.cpu().numpy())\n",
    "            adv_inputs = torch.FloatTensor(adv_inputs).to(self.device)\n",
    "\n",
    "            outputs = self.model(adv_inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        self.writer.add_scalar('Test/Adversarial_Accuracy', accuracy, epoch)\n",
    "        return accuracy\n",
    "\n",
    "# 6. Augmentation Methods\n",
    "class AugmentationMethods:\n",
    "    @staticmethod\n",
    "    def cutmix(x, y, alpha=1.0):\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "        x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "        y_a, y_b = y, y[index]\n",
    "        return x, y_a, y_b, lam\n",
    "\n",
    "    @staticmethod\n",
    "    def mixup(x, y, alpha=1.0):\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 7. Main Training Loop\n",
    "def main():\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter('runs/adversarial_training_experiment')\n",
    "\n",
    "    # Setup data and model\n",
    "    data_prep = DatasetPreparation()\n",
    "    trainloader, testloader = data_prep.load_data()\n",
    "\n",
    "    model_setup = ModelSetup()\n",
    "    model = model_setup.model\n",
    "\n",
    "    # Initialize trainer and evaluator\n",
    "    trainer = ModelTrainer(model, trainloader, testloader, writer)\n",
    "    evaluator = RobustnessEvaluator(model, testloader, writer)\n",
    "\n",
    "    # Create ART classifier\n",
    "    art_classifier = model_setup.get_art_model(\n",
    "        model, trainer.criterion, trainer.optimizer)\n",
    "\n",
    "    # Initialize attacks\n",
    "    attacks = AdversarialAttacks(art_classifier)\n",
    "    fgsm_attack = attacks.create_fgsm_attack()\n",
    "    pgd_attack = attacks.create_pgd_attack()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        train_loss, train_acc = trainer.train_epoch(epoch)\n",
    "\n",
    "        # Evaluate on clean and adversarial examples\n",
    "        clean_acc = evaluator.evaluate_clean(epoch)\n",
    "        fgsm_acc = evaluator.evaluate_adversarial(fgsm_attack, epoch)\n",
    "        pgd_acc = evaluator.evaluate_adversarial(pgd_attack, epoch)\n",
    "\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}%')\n",
    "        print(f'Clean Acc: {clean_acc:.3f}% | FGSM Acc: {fgsm_acc:.3f}% | PGD Acc: {pgd_acc:.3f}%')\n",
    "\n",
    "        trainer.scheduler.step()\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "79f6d7d93d14bc98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 17:20:41.375500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:37<00:00, 4542762.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lastjan/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/lastjan/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/lastjan/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:19<00:00, 5.18MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 237\u001B[0m\n\u001B[1;32m    234\u001B[0m     writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 237\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 221\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    218\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m200\u001B[39m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# Train one epoch\u001B[39;00m\n\u001B[0;32m--> 221\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m# Evaluate on clean and adversarial examples\u001B[39;00m\n\u001B[1;32m    224\u001B[0m     clean_acc \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate_clean(epoch)\n",
      "Cell \u001B[0;32mIn[6], line 88\u001B[0m, in \u001B[0;36mModelTrainer.train_epoch\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m     86\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(inputs)\n\u001B[1;32m     87\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, targets)\n\u001B[0;32m---> 88\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     91\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:38:50.342973Z",
     "start_time": "2024-12-26T19:38:50.339662Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e7e13ee6f65ce206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "654c86d8da2fd1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
